---
title : "CS234 7강 Summary"
excerpt: "CS234 강의 중 Lecture 7(Imitation Learning)을 듣고 정리해보자"

category :
    - CS234
tag :
    - CS234

toc : true
toc_sticky: true
comments: true

---

CS234 강의 중 Lecture 7(Imitation Learning)을 듣고 정리해보자

> 본 포스팅은 Stanford Univ의 [CS234:Reinforcement Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) 강의를 듣고 정리한 글입니다.

## Why we Need Imitation Learning

일반적인 MDP에서는, 좋은 policy를 찾기 위해서는 굉장히 많은 양의 sample들이 필요했다. 가령 DQN같은 경우, 굉장히 오랫동안 훈련시켜야, 즉 최대한 많은 양의 sample들이 있어야 좋은 성적을 낼 수 있었다.


그런데, 실제 강화 학습의 경우 이런 sample들을 얻기란 쉽지 않다.

예를 들어, 자율주행 자동차의 경우 수없이 많은 사고 이후에야 운전을 제대로 할 수 있을 것이다. 하지만 이를 위해 생기는 비용 등의 부정적인 측면으로 인해 Real World에서 단순히 RL만을 활용하는 것은 아직까지 약간의 무리가 있다.

더욱 디테일 하게 말해서 지금까지 우리들은 RL에서 Reward를 통해서 Agent를 학습시켰었다. DQN, Q-learning, MC 등등 모두 다 reward function을 사용하여 최대의 reward를 얻을 수 있도록 하는 것이 주요 포인트였다. 이 방식은 매우 간단한 방식으로 훈련이 가능하다는 점에서 좋지만, 아까 위에서 언급했듯 너무 많은 sample을 요구한다는 단점이 있다.

Reward를 산정하는 방식도 조금 생각해보자. 가령 자율 주행 자동차의 reward를 산정하려면 어떻게 해야할까?

만약 이 reward를 사고날 때 -10, 안나면 +0.1 이런 식으로 설정한다면 Agent는 어떻게 해야 사고가 나고 어떻게 해야 사고가 나지 않을지 알아내느라 한참을 고생할 것이다.

반대로 reward를 모든 상황에 대해서 적절하게 설정하려고 한다면 너무 오랜 시간이 걸리기도 하고(1초 마다 reward를 넣어주어도 1시간 이상 운전한다면..) reward의 상태가 매우 불안정해질 수 있다

따라서 이러한 문제를 해결하기 위해 도입된 것이 reward를 demonstration, 즉 실제로 어떻게 하는지 보여주면서 reward를 implicit하게 주는 **Imitation Learning**이다.

이렇게 demonstration으로 reward를 산정하려면 어떻게 해야할까? 바로 학습시킬 일의 전문가를 데려와서 demonstration trajectory를 만들어 학습시키는 것이다.

가령 자율 주행 자동차를 만든다고 한다면, 운전을 매우 잘하는 어떤 사람을 데려와서 실제로 한번 운전시켜 보는 것이다.

그렇게 얻은 State/action sequence들을 강화 학습 Agent에게 줌으로써 그것을 바탕으로 학습시키면 된다.

이러한 Imitation Learning 방식은 강화학습 과정을 도와줄 추가적인 정보나 구조들을 알려주어 **학습의 효율성을 높여주며**, reward를 일일히 부여하거나 특정 policy를 따르도록 하게 하려는 것이 아닐 경우에 효율적이다.

## Imitation Learning 종류

이제부터 Imitation Learning의 종류인 Behavioral Cloning, Inverse RL, Apprenticeship learning에 대해 배울 것인데, 각 종류에 대한 목표 과제는 다음과 같다.

- **Behavioral Cloning** : supervised learning을 통해 스승(전문가)의 policy를 직접 배울 수 있게 하자!

- **Inverse RL** : reward function R을 demonstration을 통해 얻을 수 있을까?

- **Apprenticeship learning** : R값을 좋은 policy를 생성하는데 사용할 수 있나?

### Behavioral Cloning

Behavioral Cloning 방식은 기존의 머신 러닝 문제를 풀기 위해 자주 사용하던 Supervised learning 방식을 사용하는 것이다.

방법은 다음과 같다.

우선 policy class를 설정해 두고, (인공 신경망, Decision tree 등등을 사용할 수 있다) expert의 state와 action의 sequence를 supervised learning model의 input/output으로 두고 Agent를 학습시킨다.

자율 주행 자동차를 예시로 들자면, 만약 왼쪽으로 코너를 돌 때의 action이 대부분 핸들을 왼쪽으로 꺾는 것이라면, supervised learning model은 다음부터 왼쪽으로 돌아야 한다는 state를 받을 때 action은 핸들을 왼쪽으로 꺾어야 한다고 학습할 것이다.

<p align="center"><img src="https://github.com/jebeom/jebeom.github.io/assets/107978090/1038cc19-32e6-4868-b99a-90783c33e020" ></p>

하지만, Behavioral Cloning 방식에는 Compounding Error라는 큰 문제가 하나 있다.

이는 Supervised learning은 모든 데이터가 iid라는 것을 전제로 한다는 것 때문에 발생한다. 여기서 iid란 각각의 데이터들이 독립적이고 동일한 확률분포를 가진다는 것을 의미하는데, 조금 더 쉽게 하자면 그냥 모든 데이터 아무거나 하나를 뽑아도 다른 데이터와 연관이 없다는 것을 전제한다는 것이다. (그래서 일반적인 기계 학습에서는 데이터를 셔플 하는 등 데이터의 순서를 마구 섞어서 input으로 집어넣어도 된다.)

그런데, 분명 우리가 주는 데이터 state, action pair은 **시간의 흐름에 따라 이어지는 데이터** e로 순서가 중요하다.

하지만 Supervised learning에서는 이러한 데이터의 시간적 구조를 싸그리 무시하고, 모든 데이터를 iid라고 가정하기 때문에, 언제 어떤 state였는지는 중요하지 않고, 그냥 특정 state에서 특정 action이 취해지길 바라고 있는 것이다.
 
예를 들어, 위 사진처럼 Expert가 운전한 대로 Agent가 운전을 하고 있다가, 특정 구간에서의 Error로 인해 코너링 초반에 실수를 조금 했다고 가정하자. (조금 더 바깥쪽으로 코너링을 시도했다.)

하지만, expert는 그런 실수를 하지 않았기에, (그리고 하지도 않을 것이기에,) 저런 상황에서 어떻게 복구해야 하는지 알 길이 없다.

그러면 time step t에서의 실수 하나로 인해 그 이후의 time step t+1, t+2에서도 계속 error가 생길 수 밖에 없고, 그러다 보면 결국 학습에 실패하게 되는 것이다.

<p align="center"><img src="https://github.com/jebeom/jebeom.github.io/assets/107978090/5f065790-e34f-4afc-9a7f-dab9cf6ac498" ></p> 

이 문제를 해결하기 위해 DAGGER : Dataset Aggregation이라는 방식을 사용한다.

DAGGER 방식은 잘못된 길을 가는 경우 expert에게 어떤 action을 취해야 할지 알려달라고 하는 것이다. 즉, 아까 전에 들었던 예시처럼, 코너링을 잘못 돌았을 때, agent가 expert에게 '이런 경우에는 어떻게 해야하나요?' 라고 물어보고, expert가 해당 상황에서는 어떤 action을 취해야 하는지 알려주는 것이다.

아이디어만 들어도 알겠지만, 이 방식은 모든 상황에서 효율적으로 쓰일 수 있는 방식은 아니다.

우선, 정말 짧은 time step 간의 state에 대한 action이 필요한 경우에는 사실상 이러한 방식이 불가능하다. 예를 들어, 자율 주행 자동차 같은 경우, 정말 짧은 시간동안 어떻게 운전할지가 중요한데, 이런 경우에는 모든 잘못된 경우마다 어떻게 해야 하는지 알려주는 것은 힘들 것이다.

이러한 단점으로 인해, 후술할 다른 방식 보다는 잘 사용하지 않고 있다.

### Inverse Reinforcement Learning

다음으로 알아볼 방식은 Inverse RL이라는 방식이다.

이 방식은 expert의 policy를 보고, reward function을 찾아나가는 방식이다.

우선 imitation learning의 세팅은 지금까지의 RL과 비슷하게 state space, action space, transition model P가 주어지지만, **reward function R은 주어지지 않는다.** 대신 expert의 demonstration $(s_{1},a_{1},s_{2} ...)$을 받게 된다. 

Inverse RL은 expert의 demonstration을 통해 reward function R을 알아가게 된다. 다시 말해서, RL이 주어진 reward function을 통해 optimal policy 를 계산한다면, Inverse RL은 optimal policy(최적 정책, 곧 행동 이력) 를 입력으로 reward function을 계산한다.

여기서 간단하게 질문 하나를 해보자. 만약, expert의 policy가 optimal하다는 전제가 없다면, 위 demonstration은 R에 대한 어떤 정보를 줄 수 있을까 ?

정답은 '줄 수 없다' 이다.

만약 expert의 policy가 최적이 아닌 별로인 policy라고 생각해보자. 즉, 실생활에서 예를 들자면 expert가 자동차를 운전할 때 그냥 직진만 한다고 해보자. 그러면 expert가 운전하는 모습을 보고 운전을 처음 하는 사람이 뭔가를 배울 수 있을까 ?

당연하게도 알 수 없다.

즉, 우리가 이 Inverse RL 방식을 사용하려면 expert의 policy는 optimal하다는 전제가 있어야 한다.

그렇다면, expert의 policy가 optimal하다고 가정했을 때 reward function은 unique할까, 아니면 여러 개가 있을 수 있을까 ? (단, 데이터는 충분히 존재한다고 가정한다.)

정답은, **여러 개가 존재할 수 있다** 이다.

이유는 간단하다. 그냥 모든 action에 따른 reward에다가 0이나 1,2,3 등과 같은 동일한 상수값을 할당한다면, 어떤 optimal policy가 있더라도(즉 어떤 action을 취하더라도) 모두 다 동일한 reward를 가지게 되므로 모든 policy가 optimal 해질 것 이다.

이런 점에서, inverse RL은 큰 문제에 봉착하게 되었다.



<p align="center"><img src="https://github.com/jebeom/jebeom.github.io/assets/107978090/5c0feded-73b0-4603-83cb-e222b9527ca2" ></p>

그렇다면 이 Inverse RL의 문제를 어떻게 해결할까?

우선 저번에 배웠던 Linear value function approximation을 다시 가져와 보자.

R값은 여러 개가 존재할 수도 있다고 했지만, 그냥 일단 R값을 $w^{T} x(s)$ 라고 해보자.(이 때, w는 weight vector이고 x(s)는 state의 feature이다.) 

우리의 목표는 이 weight vector w를 주어진 demonstration을 통해 찾아내는 것이 목표이다.

위 슬라이드에서의 수식에서 $\mu(\pi)(s)$가 의미하는 바가 무엇일까? (참고로 $\mu(\pi)$는 벡터이기에 (s)가 달리며, 중간에 $w^{T}$가 앞으로 나온 이유는 모든 $\pi$에 대하여 $w^{T}$ 가 동일하기 때문이다. )

$\mu(\pi)(s)$ 는 각각의 time step t에서 나타나는 state feature x(s)에다가 discount factor $\gamma^{t}$를 곱한 것이다.

그리고 거기에다가 w의 transpose를 곱하므로, value function $V^{\pi}$ 는 각 state feature의 weighted discounted frequency를 나타내는 값과 동일해진다. 즉, 우리가 학습시킨 weight vector w값에다가 자주 등장하는 state feature의 값을 곱해주는 것이다.


자, 그럼 이 값이 잘 만들어진 값인지 보자!

Inverse RL에서 우리의 목표는 (optimality가 전제된) expert의 policy를 보고, 그 policy를 토대로 reward function이 어떻게 되어 있을지 찾아가는 것이었다.

아까 전에 state feature x(s)에다가 discount factor $\gamma^{t}$를 곱해주었기에 expert의 policy에서 자주 보이는 state feature의 실제 reward 값은 높고, 거의 보이지 않았던 state feature를 갖는 state의 reward값은 일반적으로 낮게 될 것이다.

즉, 한번 나왔던 state feature가 시간이 지나도(t가 증가해도) discount factor $\gamma$ 가 곱해져서 또 다시 나와서 더해지기에 자주 나오는 state의 reward는 자연스레 높아지는 것이다.

따라서 아까 reward값이 동일해서 어떤 policy라도(최악의 policy를 포함한) optimal하게 되는 문제를 **자주 나오는 state마다 reward에 차이를 주어** 어느 정도 해결이 가능하다.

 

### Apprenticeship Learning

다음 방식은 Apprenticeship Learning이다.

<p align="center"><img src="https://github.com/jebeom/jebeom.github.io/assets/107978090/25ddd2ed-d581-47f9-99c8-d6b0404baed4" ></p>

사실 Apprenticeship Learning도 Inverse RL과 굉장히 비슷하다. 방금 전까지 한거 그대로 이어서, 조금 더 잘 만든 버전이라고 생각하면 될 듯 하다. ($V^{\pi} = w^{T} \mu(\pi)$ 까지는 동일함)

위 슬라이드를 보면, 최적의 V 값인 $V^* $는 언제나 $V^{\pi}$ 보다 크거나 같다. 그러므로, expert의 demonstration이 optimal policy에서 온 것이라면, w를 찾기 위해서는 위의 슬라이드에서 (6)번 식을 만족하는 $w^* $를 찾을 필요가 있다.

해당 수식에서 $\mu (\pi^* )$는 expert가 주는 optimal한 policy이므로 우리가 이미 알고 있는 값이고,

 $\mu (\pi)$는 expert가 주는 policy를 제외한 다른 어떤 policy를 의미한다. 

즉, optimal policy의 값을 정말 optimal하게 만들어주는 $w^* $ 의 값을 찾아야 한다는 것이다. (만약 위의 수식을 만족하지 않는다면 optimal policy보다 높은 V값을 가지는 policy가 존재한다는 것인데, 이건 말이 안된다..)

이는 expert policy가 다른 어떤 policy보다 잘 작동하게 하는 reward function을 찾고 싶다는 것을 의미하며, 만약 우리의 policy $\pi$ 와 expert policy $\pi^ *$ 에서의 Value 즉, V값이 충분히 비슷하다면 우리는 optimal policy 수준의 policy를 찾아냈다고 할 수 있을 것이다.


<p align="center"><img src="https://github.com/jebeom/jebeom.github.io/assets/107978090/5cc47d30-f38c-4dd4-af0f-793fb21a115a" ></p>

이를 위해 우리는 위 슬라이드와 같이 (7)번식을 만족하는 $\pi$와 그 아래 식을 만족하는 $w$를 구해야 한다. ($w$값의 절대값이 1보다 작아야 하는 이유는 훈련 도중에 값이 explode하지 않게 하기 위함인듯 하다.)


이렇게 하면, 원래 reward function이 뭐였느냐에 관계 없이, 학습으로 도출된 reward function을 사용하더라도 충분히 optimal policy에 가까운 policy를 얻어낼 수 있다.

<p align="center"><img src="https://github.com/jebeom/jebeom.github.io/assets/107978090/afa5a1bb-c2b1-4bca-8eb9-38d435b5406d" ></p>

지금까지 위에서 말한 Apprenticeship Learning의 알고리즘은 위와 같다. (근데 교수님 말씀으로는 이거 이제 거의 안쓰인다고 하심)

지금까지에서 가장 중요한 것은, optimal policy와 충분히 비슷한 policy를 얻어내는 것만으로도 학습이 충분하다는 것이다. (실제 optimal policy가 가지던 reward function과 관계없이 어떠한 reward function으로라도 저런 optimal policy에 근접한 policy만 찾을 수 있으면 된다는 뜻이다.)


## 남아있는 문제점

하지만 아직 문제점들이 남아 있다.

아까 전에 같은 optimal policy에도 수없이 많은 reward function들이 있을 수 있다고 했는데, 사실 위의 알고리즘이 이 문제를 완벽히 해결하지는 못한다.


또한, reward function을 구했더라도 그 reward function에 최적으로 부합하는 policy도 사실 여러 개가 있을 것이다.

그 중 어떤 것을 골라야 하는가? 가 바로 그 문제점이다.

이런 문제들 같은 경우, 아직도 활발히 연구되고 있다.


주요 논문으로는, Maximum Entropy Inverse RL과 Generative adversarial imitation learning이 있다.

Maximum Entropy Inverse RL의 경우, Imitation learning의 uncertanty를 최소화 하기 위해, (즉, 최악의 선택을 피하기 위해) entropy를 최대화 시켜줘야 한다는 것이다. 최대한 일반적인 움직임만을 선택하자는 느낌이다.

Generative adversarial imitation learning, 줄여서 GAIL의 경우, discriminator(판별자)를 만들어서, expert policy와 우리가 찾아낸 policy를 구별하게 만드는 것이다.

그렇게 해서 discriminator가 expert policy와 그냥 policy를 구별할 수 없을 정도의 policy를 찾아낸다면, 그것이 바로 충분히 좋은 policy라고 하는 것이다.

이 방식을 사용함으로써, 우리는 통계적 계산(자주 나온 state에 높은 reward)의 산물인 $\mu(\pi)$ 에서 조금 더 멀어져서, 실제 좋은 움직임을 찾을 수 있을 것이다.

  
## Reference

- [강화학습 강의 (CS234) 7강 - Imitation Learning / Inverse RL](https://cding.tistory.com/71)



